{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "Text is everywhere in the form of openions, complaints, news and information; each of us are contributing in generating this text throughout the internet. This large amount of text which is present in the unstructured format can be used for welfare of the society and businesses by analysing the sentiments and trends. A part of NLP deals with Text processing and analysis. Commonly available text can not be directly used for analysis, it needs to pre-processed first, and that is where NLTK, the most popular library for text processing in Python, comes into the picture.\n\nYou may check out my previous related work where I have performed NLTK basic operations, Text Pre-processing, explained Bag of Words (BOW) and TF-IDF feature extraction techniques in detail with examples. If you already don't know how BOW and TF-IDF technique works then you must go through the first kernel atleast before proceeding any further.\n\n* _https://www.kaggle.com/amar09/text-pre-processing-and-feature-extraction_\n* _https://www.kaggle.com/amar09/nltk-feature-extraction-and-sentiment-analysis_\n\nIn this notebook, I am going to perform Sentiment Analysis on tweets on any desired topic. Instead of fetching tweets from and CSV I am going to scrap these tweets from the internet. So, get ready for a crash course on Scraping (using Selenium) and Twitter Sentiment Analysis.", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "6d7f0e211c8e305889bf40952e841a0238970739"
            }
        }, 
        {
            "source": "### Actions I am going to perform are:\n\n\n1. __<a href='#1' target='_self'>Import Libraries</a>__\n1. __<a href='#2' target='_self'>Fetch Tweets & Sentiments</a>__\n    1. __<a href='#2A' target='_self'>Fetch Tweets</a>__\n        1. <a href='#2Aa' target='_self'>Using Selenium</a>\n        1. <a href='#2Ab' target='_self'>Using tweepy</a>\n    1. __<a href='#2B' target='_self'>Fetch sentiments</a>__\n        1. <a href='#2Ba' target='_self'>Using NLTK's SentimentIntensityAnalyzer</a>\n        1. <a href='#2Bb' target='_self'>Using TextBlob</a>\n1. __<a href='#3' target='_self'>Text Pre-processing</a>__\n    1. __<a href='#3A' target='_self'>Pre-processing 'Key Words'</a>__\n        1. <a href='#3Aa' target='_self'>Removing '@names'</a>\n        1. <a href='#3Ab' target='_self'>Removing links (http | https)</a>\n        1. <a href='#3Ac' target='_self'>Removing tweets with empty text</a>\n        1. <a href='#3Ad' target='_self'>Dropping duplicate rows</a>\n        1. <a href='#3Ae' target='_self'>Removing Punctuations, Numbers and Special characters</a>\n        1. <a href='#3Af' target='_self'>Removing Stop words</a>\n        1. <a href='#3Ag' target='_self'>Tokenizing</a>\n        1. <a href='#3Ah' target='_self'>Converting words to Lemma </a>\n        1. <a href='#3i' target='_self'>Joining all tokens into sentences</a>\n    1. __<a href='#3B' target='_self'>Pre-processing 'Key Phrases'</a>__\n        1. <a href='#3Ba' target='_self'>Setting Grammatical rule to identify phrases</a>\n        1. <a href='#3Bb' target='_self'>Creating new feature called 'key_phrases', will contain phrases for corresponding tweet</a>\n1. __<a href='#4' target='_self'>Story Generation and Visualization</a>__\n    1. __<a href='#4A' target='_self'>Most common words in positive tweets</a>__\n    1. __<a href='#4B' target='_self'>Most common words in negative tweets</a>__\n    1. __<a href='#4C' target='_self'>Most commonly used Hashtags</a>__\n1. __<a href='#5' target='_self'>Feature Extraction</a>__\n    1. __<a href='#5A' target='_self'>Feature Extraction for 'Key Words'</a>__\n    1. __<a href='#5B' target='_self'>Feature Extraction for 'Key Phrases'</a>__\n1. __<a href='#6' target='_self'>Model Building: Sentiment Analysis</a>__\n    1. __<a href='#6A' target='_self'>Predictions on 'key words' based features</a>__\n        1. <a href='#6Aa' target='_self'> BOW word features</a>\n        1. <a href='#6Ab' target='_self'>TF-IDF word features</a>\n    1. __<a href='#6B' target='_self'>Predictions on 'key phrases' based features</a>__\n        1. <a href='#6Ba' target='_self'>BOW phrase features</a>\n        1. <a href='#6Bb' target='_self'>TF-IDF phrase features</a>\n       ", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "b9f81e542e81a4ee1b293ac970ef56e1e447543c"
            }
        }, 
        {
            "source": "## <a id='1'>1. Import Libraries</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "1855c583de74a56d816884d34bdc5b5220887c37"
            }
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {
                "_kg_hide-input": true, 
                "_uuid": "e727f86049f254ca4496b3d3a57d7135751501ca", 
                "_kg_hide-output": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Requirement not upgraded as not directly required: selenium in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\nRequirement not upgraded as not directly required: urllib3 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from selenium)\nRequirement not upgraded as not directly required: tweepy in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\nRequirement not upgraded as not directly required: requests>=2.11.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from tweepy)\nRequirement not upgraded as not directly required: six>=1.10.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from tweepy)\nRequirement not upgraded as not directly required: PySocks>=1.5.7 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from tweepy)\nRequirement not upgraded as not directly required: requests-oauthlib>=0.7.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from tweepy)\nRequirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests>=2.11.1->tweepy)\nRequirement not upgraded as not directly required: idna<2.7,>=2.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests>=2.11.1->tweepy)\nRequirement not upgraded as not directly required: urllib3<1.23,>=1.21.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests>=2.11.1->tweepy)\nRequirement not upgraded as not directly required: certifi>=2017.4.17 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests>=2.11.1->tweepy)\nRequirement not upgraded as not directly required: oauthlib>=0.6.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests-oauthlib>=0.7.0->tweepy)\nRequirement not upgraded as not directly required: wordcloud in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\nRequirement not upgraded as not directly required: pillow in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from wordcloud)\nRequirement not upgraded as not directly required: numpy>=1.6.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from wordcloud)\nRequirement not upgraded as not directly required: olefile in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pillow->wordcloud)\nCollecting textblob\n  Downloading https://files.pythonhosted.org/packages/7c/7d/ad09a26b63d4ad3f9395840c72c95f2fc9fa2b192094ef14e9e720be56f9/textblob-0.15.2-py2.py3-none-any.whl (636kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 645kB 1.6MB/s eta 0:00:01\n\u001b[?25hRequirement not upgraded as not directly required: nltk>=3.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from textblob)\nRequirement not upgraded as not directly required: six in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from nltk>=3.1->textblob)\nInstalling collected packages: textblob\nSuccessfully installed textblob-0.15.2\n"
                }
            ], 
            "source": "!pip install selenium\n!pip install tweepy\n!pip install wordcloud\n!pip install textblob"
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {
                "_kg_hide-output": true, 
                "_uuid": "4bd54dfb628421be771491b36e419fe7be4eba19"
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n[nltk_data] Downloading package wordnet to /home/dsxuser/nltk_data...\n[nltk_data]   Unzipping corpora/wordnet.zip.\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
                }
            ], 
            "source": "import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport re\nimport time\nimport string\nimport warnings\n\n# for all NLP related operations on text\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import *\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, confusion_matrix\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\n# To mock web-browser and scrap tweets\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\n\n# To consume Twitter's API\nimport tweepy\nfrom tweepy import OAuthHandler \n\n# To identify the sentiment of text\nfrom textblob import TextBlob\n\n# ignoring all the warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n# downloading stopwords corpus\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('vader_lexicon')\nnltk.download('averaged_perceptron_tagger')\nstopwords = set(stopwords.words(\"english\"))\n\n# for showing all the plots inline\n%matplotlib inline"
        }, 
        {
            "source": "## <a id='2'>2. Fetch Tweets & Sentiments</a>\n### <a id='2A'>A. Fetch Tweets</a>\nWe have two ways two get tweets from tweeter:<br/>\n__a. Scrap using Selenium <br/>__\n__b. Fetch from twitter API using 'tweepy' <br/>__\n\nI have created classes for both, and any of them can be used. \nSelenium takes around 10 mins to fetch approx 850 tweets which is very slow; If you still are going to use Selenium, then make sure that 'chromedriver' should be locatable, which might not be possible on any cloud notebook platform (I guess).", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "a98d8028123fe5630fa28c53cd9de314168443bc"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "c25029d9f96d44b72c2a578feb23cdbf4b48a505"
            }, 
            "outputs": [], 
            "source": "class SeleniumClient(object):\n    def __init__(self):\n        ''' \n        Initialization method. \n        '''\n        self.chrome_options = webdriver.ChromeOptions()\n        self.chrome_options.add_argument('--headless')\n        self.chrome_options.add_argument('--no-sandbox')\n        self.chrome_options.add_argument('--disable-setuid-sandbox')\n\n        # you need to provide the path of chromdriver in your system\n        self.browser = webdriver.Chrome('D:/chromedriver_win32/chromedriver', options=self.chrome_options)\n\n        self.base_url = 'https://twitter.com/search?q='\n\n    def get_tweets(self, query):\n        ''' \n        Function to fetch tweets. \n        '''\n        try: \n            self.browser.get(self.base_url+query)\n            time.sleep(2)\n\n            body = self.browser.find_element_by_tag_name('body')\n\n            for _ in range(3000):\n                body.send_keys(Keys.PAGE_DOWN)\n                time.sleep(0.3)\n\n            timeline = self.browser.find_element_by_id('timeline')\n            tweet_nodes = timeline.find_elements_by_css_selector('.tweet-text')\n\n            return pd.DataFrame({'tweets': [tweet_node.text for tweet_node in tweet_nodes]})\n\n        except: \n            print(\"Selenium - An error occured while fetching tweets.\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_kg_hide-input": true, 
                "_uuid": "b513c68eeaa0e653c399c57d1c4b76b0bc5587e9", 
                "_kg_hide-output": true
            }, 
            "outputs": [], 
            "source": "# keys and tokens from the Twitter Dev Console \nconsumer_key = 'Sec3MvclRIx2RVlgu9l0SJX6D'\nconsumer_secret = 'ayoPNWtBm7fWpMBoK6EwRmegu3SW8Rw9mzJkottkv97quPe941'\naccess_token = '736550752760406018-so5CPJrEbJKb3c3Pq8va3VFr0yk4S0E'\naccess_token_secret = 'Cgr8tz0h6FTU7kxAjDzpHnjffNTHxWsBytXnu4Ihd1TFb'"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "070a5ec569d9b5711f191928f77b6d1ad00f26f6"
            }, 
            "outputs": [], 
            "source": "class TwitterClient(object): \n    def __init__(self): \n        ''' \n        Initialization method. \n        '''\n        try: \n            # create OAuthHandler object \n            auth = OAuthHandler(consumer_key, consumer_secret) \n            # set access token and secret \n            auth.set_access_token(access_token, access_token_secret) \n            # create tweepy API object to fetch tweets \n            self.api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n            \n        except tweepy.TweepError as e:\n            print(f\"Error: Tweeter Authentication Failed - \\n{str(e)}\") \n\n    def get_tweets(self, query, maxTweets = 1000): \n        ''' \n        Function to fetch tweets. \n        '''\n        # empty list to store parsed tweets \n        tweets = [] \n        sinceId = None\n        max_id = -1\n        tweetCount = 0\n        tweetsPerQry = 100\n        \n        while tweetCount < maxTweets:\n            try:\n                if (max_id <= 0):\n                    if (not sinceId):\n                        new_tweets = self.api.search(q=query, count=tweetsPerQry)\n                    else:\n                        new_tweets = self.api.search(q=query, count=tweetsPerQry,\n                                                since_id=sinceId)\n                else:\n                    if (not sinceId):\n                        new_tweets = self.api.search(q=query, count=tweetsPerQry,\n                                                max_id=str(max_id - 1))\n                    else:\n                        new_tweets = self.api.search(q=query, count=tweetsPerQry,\n                                                max_id=str(max_id - 1),\n                                                since_id=sinceId)\n                if not new_tweets:\n                    print(\"No more tweets found\")\n                    break\n                    \n                for tweet in new_tweets:\n                    parsed_tweet = {} \n                    parsed_tweet['tweets'] = tweet.text \n\n                    # appending parsed tweet to tweets list \n                    if tweet.retweet_count > 0: \n                        # if tweet has retweets, ensure that it is appended only once \n                        if parsed_tweet not in tweets: \n                            tweets.append(parsed_tweet) \n                    else: \n                        tweets.append(parsed_tweet) \n                        \n                tweetCount += len(new_tweets)\n                print(\"Downloaded {0} tweets\".format(tweetCount))\n                max_id = new_tweets[-1].id\n\n            except tweepy.TweepError as e:\n                # Just exit if any error\n                print(\"Tweepy error : \" + str(e))\n                break\n        \n        return pd.DataFrame(tweets)"
        }, 
        {
            "source": "#### <a id='2Aa'>a. Using Selenium</a>\nWe could not use 'request' and 'BeautifuSoup' because in case of twitter tweet-feed comes dynamically (from JavaScript) in a progressively manner (infinite load). So we are using Selenium, it can mock the browser behaviour.", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "5d7de96a6f8f096d941722e690e77b5313dc6f14"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "11b78f89cee0fcdc7712a64317615b8814d2e72b"
            }, 
            "outputs": [], 
            "source": "# selenium_client = SeleniumClient()\n\n# # calling function to get tweets\n# tweets_df = selenium_client.get_tweets('AI and Deep learning')\n# print(f'tweets_df Shape - {tweets_df.shape}')\n# tweets_df.head(10)"
        }, 
        {
            "source": "#### <a id='2Ab'>b. Using 'tweepy'</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "ef7aa0669142814e1812d2beec0baee5c6a0dd9c"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "ebceda95b0ca3314ead3395ba1d9fbdfb3edfc23"
            }, 
            "outputs": [], 
            "source": "twitter_client = TwitterClient()\n\n# calling function to get tweets\ntweets_df = twitter_client.get_tweets('Travis Translator', maxTweets=5000)\nprint(f'tweets_df Shape - {tweets_df.shape}')\ntweets_df.head(10)"
        }, 
        {
            "source": "### <a id='2B'>B. Fetch sentiments</a>\nTo proceed further, we need to know the sentiment type of every tweet, that can be done using two ways: <br/>\n    __a. Using NLTK's SentimentIntensityAnalyzer (We'll refer as SIA)<br/>__\n    __b. Using TextBlob<br/>__", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "546cfdb0f0701d4a70751a87c9573724a444308f"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "b6d49397ef4f9a9a9dc852a7a6d55b226ba0aaf2"
            }, 
            "outputs": [], 
            "source": "# 1\ndef fetch_sentiment_using_SIA(text):\n    sid = SentimentIntensityAnalyzer()\n    polarity_scores = sid.polarity_scores(text)\n    if polarity_scores['neg'] > polarity_scores['pos']:\n        return 'negative'\n    else:\n        return 'positive'\n\n# 2\ndef fetch_sentiment_using_textblob(text):\n    analysis = TextBlob(text)\n    # set sentiment \n    if analysis.sentiment.polarity >= 0:\n        return 'positive'\n    else: \n        return 'negative'"
        }, 
        {
            "source": "#### <a id='2Ba'>a. Using NLTK's SentimentIntensityAnalyzer</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "3faa005eb91dd55fa73eb85250ecc55f6f79debb"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "bf6ed6d0ce448ab148ae07a92435efbbe6343ca0"
            }, 
            "outputs": [], 
            "source": "sentiments_using_SIA = tweets_df.tweets.apply(lambda tweet: fetch_sentiment_using_SIA(tweet))\npd.DataFrame(sentiments_using_SIA.value_counts())"
        }, 
        {
            "source": "#### <a id='2Bb'>b. Using TextBlob</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "9e5a3804eb76da3286663fb4dbf3631a1f42b626"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "a9551eea136f0f0857ed692191a7b83608b61db6"
            }, 
            "outputs": [], 
            "source": "sentiments_using_textblob = tweets_df.tweets.apply(lambda tweet: fetch_sentiment_using_textblob(tweet))\npd.DataFrame(sentiments_using_textblob.value_counts())"
        }, 
        {
            "source": "TextBlob gives us more negative sentiments than SIA, so we will prefer textblob, since classfication seems better.", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "70a46c74011dc5945b134ee735db118b60f3e51e"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "2e1915dd45c5d4bef21fb9d80737f49e2a27cd6e"
            }, 
            "outputs": [], 
            "source": "tweets_df['sentiment'] = sentiments_using_textblob\ntweets_df.head()"
        }, 
        {
            "source": "## <a id='3'>3. Text Pre-processing</a> \n### <a id='3A'>A. Pre-processing 'Key Words'</a>\n#### <a id='3Aa'>a. Removing '@names'</a>\nHere we can see that at many places we have '@names', which is of no use, since it don't have any meaning, So needs to be removed.", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "b818211e308d31bd3642fccd49db3c88445869ee"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "52d22074e346cdb7c1f8a904f02f32e9354842bf"
            }, 
            "outputs": [], 
            "source": "def remove_pattern(text, pattern_regex):\n    r = re.findall(pattern_regex, text)\n    for i in r:\n        text = re.sub(i, '', text)\n    \n    return text "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "a4d6ff8b3407f4bbb062817d0990609e9e5d80b4"
            }, 
            "outputs": [], 
            "source": "# We are keeping cleaned tweets in a new column called 'tidy_tweets'\ntweets_df['tidy_tweets'] = np.vectorize(remove_pattern)(tweets_df['tweets'], \"@[\\w]*: | *RT*\")\ntweets_df.head(10)"
        }, 
        {
            "source": "Seems good, now we don't have '@name' values", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "397e01e06fbca16bae60bec5809ec52827fbda83"
            }
        }, 
        {
            "source": "#### <a id='3Ab'>b. Removing links (http | https)</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "f1abd70be6b8813d394cf7305a18e966723e96cb"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "9cafbb4fe878f40bdb684c0315b556cbbdc73ea8"
            }, 
            "outputs": [], 
            "source": "cleaned_tweets = []\n\nfor index, row in tweets_df.iterrows():\n    # Here we are filtering out all the words that contains link\n    words_without_links = [word for word in row.tidy_tweets.split() if 'http' not in word]\n    cleaned_tweets.append(' '.join(words_without_links))\n\ntweets_df['tidy_tweets'] = cleaned_tweets\ntweets_df.head(10)"
        }, 
        {
            "source": "#### <a id='3Ac'>c. Removing tweets with empty text</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "9805bc74ec4ba1f2129de45c203a8aab07b5a5a8"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "6262114145a768d6e8ba472256daf06436fa724a"
            }, 
            "outputs": [], 
            "source": "tweets_df = tweets_df[tweets_df['tidy_tweets']!='']\ntweets_df.head()"
        }, 
        {
            "source": "#### <a id='3Ad'>d. Dropping duplicate rows</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "565c800d54305c228094c1095886b4446acc2749"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "9c49e7e552715852ec38f935b7404fdf314a29be"
            }, 
            "outputs": [], 
            "source": "tweets_df.drop_duplicates(subset=['tidy_tweets'], keep=False)\ntweets_df.head()"
        }, 
        {
            "source": "#### <a id='3Ae'>e. Resetting index</a>\nIt seems that our index needs to be reset, since after removal of some rows, some index values are missing, which may cause problem in future operations.", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "c693ab0378dfa6c8fb465f9dedd9076b9b90f590"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "2d2b7e5ae782ad3c4c87d5251bbbbd691ceb0a09"
            }, 
            "outputs": [], 
            "source": "tweets_df = tweets_df.reset_index(drop=True)\ntweets_df.head()"
        }, 
        {
            "source": "#### <a id='3Af'>f. Removing Punctuations, Numbers and Special characters</a>\nThis step should not be followed if we also want to do sentiment analysis on __key phrases__ as well, because semantic meaning in a sentence needs to be present. So here we will create one additional column 'absolute_tidy_tweets' which will contain absolute tidy words which can be further used for sentiment analysis on __key words__.", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "fc6488639db263e956a417f95008adaa039f592f"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "084d8a854032f85db0a93415ac23a63d9cd85557"
            }, 
            "outputs": [], 
            "source": "tweets_df['absolute_tidy_tweets'] = tweets_df['tidy_tweets'].str.replace(\"[^a-zA-Z# ]\", \"\")"
        }, 
        {
            "source": "#### <a id='3Ag'>g. Removing Stop words</a>\nWith the same reason we mentioned above, we won't perform this on 'tidy_tweets' column, because it needs to be used for __key_phrases__ sentiment analysis.", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "fd330297058d3b57849b3e2071e5d590975fc37c"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "44d8377af647f4970e30389425d9c73866972bc5"
            }, 
            "outputs": [], 
            "source": "stopwords_set = set(stopwords)\ncleaned_tweets = []\n\nfor index, row in tweets_df.iterrows():\n    \n    # filerting out all the stopwords \n    words_without_stopwords = [word for word in row.absolute_tidy_tweets.split() if not word in stopwords_set]\n    \n    # finally creating tweets list of tuples containing stopwords(list) and sentimentType \n    cleaned_tweets.append(' '.join(words_without_stopwords))\n\ntweets_df['absolute_tidy_tweets'] = cleaned_tweets\ntweets_df.head(10)"
        }, 
        {
            "source": "#### <a id='3Ah'>h. Tokenize *'absolute_tidy_tweets'*</a>  ", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "dd4043008f0a1d0946c35366ee89a043301190f7"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "58d11b5a62e65af76ac3407c2e997a746657e23b"
            }, 
            "outputs": [], 
            "source": "tokenized_tweet = tweets_df['absolute_tidy_tweets'].apply(lambda x: x.split())\ntokenized_tweet.head()"
        }, 
        {
            "source": "#### <a id='3Ai'>i. Converting words to Lemma</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "e7c06fb66f27684fca7efb59a15fb3a18ac00b4a"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "33ecf629896cfd625dd4791847294120764909fa"
            }, 
            "outputs": [], 
            "source": "word_lemmatizer = WordNetLemmatizer()\n\ntokenized_tweet = tokenized_tweet.apply(lambda x: [word_lemmatizer.lemmatize(i) for i in x])\ntokenized_tweet.head()"
        }, 
        {
            "source": "#### <a id='3Aj'>j. Joining all tokens into sentences</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "9c473be345e25f0797a4626434e7b8bcab9ebdac"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "4f55e345890412e471138ee8f19c7a9f95982cf1"
            }, 
            "outputs": [], 
            "source": "for i, tokens in enumerate(tokenized_tweet):\n    tokenized_tweet[i] = ' '.join(tokens)\n\ntweets_df['absolute_tidy_tweets'] = tokenized_tweet\ntweets_df.head(10)"
        }, 
        {
            "source": "### <a id='3B'>B. Pre-processing 'Key Phrases'</a> ", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "4d0ec47ca4da414abeb8b6a9b09f1c8f07999914"
            }
        }, 
        {
            "source": "#### <a id='3Ba'>a. Helper class, will help in preprocessing phrase terms</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "d3076f4fa6cd994dbf8c755487c89328b65f6cf3"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "c570d1bbf879e4441f674d39e027092ff2139d20"
            }, 
            "outputs": [], 
            "source": "class PhraseExtractHelper(object):\n    def __init__(self):\n        self.lemmatizer = nltk.WordNetLemmatizer()\n        self.stemmer = nltk.stem.porter.PorterStemmer()\n    \n    def leaves(self, tree):\n        \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n        for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n            yield subtree.leaves()\n\n    def normalise(self, word):\n        \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n        word = word.lower()\n        # word = self.stemmer.stem_word(word) # We will loose the exact meaning of the word \n        word = self.lemmatizer.lemmatize(word)\n        return word\n\n    def acceptable_word(self, word):\n        \"\"\"Checks conditions for acceptable word: length, stopword. We can increase the length if we want to consider large phrase\"\"\"\n        accepted = bool(3 <= len(word) <= 40\n            and word.lower() not in stopwords\n            and 'https' not in word.lower()\n            and 'http' not in word.lower()\n            and '#' not in word.lower()\n            )\n        return accepted\n\n    def get_terms(self, tree):\n        for leaf in self.leaves(tree):\n            term = [ self.normalise(w) for w,t in leaf if self.acceptable_word(w) ]\n            yield term"
        }, 
        {
            "source": "#### <a id='3Bb'>b. Grammatical rule to identify phrases</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "57bc5080e2669771007618bf7e72129ab21a622f"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "9a0b3acfc88018619e4b6ff37b2b80e9128e8fb2"
            }, 
            "outputs": [], 
            "source": "sentence_re = r'(?:(?:[A-Z])(?:.[A-Z])+.?)|(?:\\w+(?:-\\w+)*)|(?:\\$?\\d+(?:.\\d+)?%?)|(?:...|)(?:[][.,;\"\\'?():-_`])'\ngrammar = r\"\"\"\n    NBAR:\n        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n        \n    NP:\n        {<NBAR>}\n        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n\"\"\"\nchunker = nltk.RegexpParser(grammar)"
        }, 
        {
            "source": "#### <a id='3Bc'>c. New feature called 'key_phrases', will contain phrases for corresponding tweet</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "558e9d4e559da16c3bb00bc7de3716dc54e260c0"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "9472947047528a3d844d8558b49d8152958d49e2"
            }, 
            "outputs": [], 
            "source": "key_phrases = []\nphrase_extract_helper = PhraseExtractHelper()\n\nfor index, row in tweets_df.iterrows(): \n    toks = nltk.regexp_tokenize(row.tidy_tweets, sentence_re)\n    postoks = nltk.tag.pos_tag(toks)\n    tree = chunker.parse(postoks)\n\n    terms = phrase_extract_helper.get_terms(tree)\n    tweet_phrases = []\n\n    for term in terms:\n        if len(term):\n            tweet_phrases.append(' '.join(term))\n    \n    key_phrases.append(tweet_phrases)\n    \ntweets_df['key_phrases'] = key_phrases\ntweets_df.head()"
        }, 
        {
            "source": "## <a id='4'>4. Story Generation and Visualization</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "2cfa83125abc895cb0f64aa7e758959277b772b7"
            }
        }, 
        {
            "source": "#### <a id='4A'>A. Most common words in positive tweets</a>\nAnswer can be best found using WordCloud", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "83636be1d62ee637f6c7f6432ef10d1c2771c43b"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "a4f05c1494856dc84dba4eb7ea241d7403f7605c"
            }, 
            "outputs": [], 
            "source": "def generate_wordcloud(all_words):\n    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=100, relative_scaling=0.5, colormap='Dark2').generate(all_words)\n\n    plt.figure(figsize=(14, 10))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis('off')\n    plt.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "feb33eae81e5810a1b867424da13a901e22e873d"
            }, 
            "outputs": [], 
            "source": "all_words = ' '.join([text for text in tweets_df['tidy_tweets'][tweets_df.sentiment == 'positive']])\ngenerate_wordcloud(all_words)"
        }, 
        {
            "source": "#### <a id='4B'>B. Most common words in negative tweets</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "0acc343318c1a795ef3f4ed8404144bbabd1669a"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "6e1a655ee8c73234cbe35248aa4be3959981b603"
            }, 
            "outputs": [], 
            "source": "all_words = ' '.join([text for text in tweets_df['tidy_tweets'][tweets_df.sentiment == 'negative']])\ngenerate_wordcloud(all_words)"
        }, 
        {
            "source": "- One thing to notice here is that word _'deep learning'_ has been used more than _'Machine Learning', 'AI', 'Artificial Intelligence', 'BigData'_ and _'DataScience'_, which shows the buzz of deep learning these days.\n\n- AI, DeepLearning and MachineLearning have clearly been used in both positive and negative tweets, which is very obvious.\nHowever, in negative tweets we can also see negative words as well like *stop, wrong, Unfortunately, fall, Cancer, shit, Greed, pressure, problem, complex, biased, never etc.*", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "e9b79f20bd718dccc2db8977febfd85b8620a380"
            }
        }, 
        {
            "source": "#### <a id='4C'>C. Most commonly used Hashtags</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "775f4761edf2abcf73d8d03dc45a39d2a3cb56cd"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "c96c61ed84d9ade72eb3b0ce72061eaa9bf5edaa"
            }, 
            "outputs": [], 
            "source": "# function to collect hashtags\ndef hashtag_extract(text_list):\n    hashtags = []\n    # Loop over the words in the tweet\n    for text in text_list:\n        ht = re.findall(r\"#(\\w+)\", text)\n        hashtags.append(ht)\n\n    return hashtags\n\ndef generate_hashtag_freqdist(hashtags):\n    a = nltk.FreqDist(hashtags)\n    d = pd.DataFrame({'Hashtag': list(a.keys()),\n                      'Count': list(a.values())})\n    # selecting top 15 most frequent hashtags     \n    d = d.nlargest(columns=\"Count\", n = 25)\n    plt.figure(figsize=(16,7))\n    ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n    plt.xticks(rotation=80)\n    ax.set(ylabel = 'Count')\n    plt.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "06f44a16d07c30ff0e5facd4ea21d5ee8aea2bfc"
            }, 
            "outputs": [], 
            "source": "hashtags = hashtag_extract(tweets_df['absolute_tidy_tweets'])\nhashtags = sum(hashtags, [])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "32cdd6e49a043bca2a624cea3e04a26bf3877fbc"
            }, 
            "outputs": [], 
            "source": "generate_hashtag_freqdist(hashtags)"
        }, 
        {
            "source": "As expected, AI, DeepLearning and MachineLearning are most common hashtags.", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "868eee2bc2cb07bce6299f60cb966bca678264a4"
            }
        }, 
        {
            "source": "## <a id='5'>5. Feature Extraction</a>\n\nWe need to convert textual representation in the form on numeric features. We have two popular techniques to perform feature extraction:\n\n1. __Bag of words (Simple vectorization)__\n2. __TF-IDF (Term Frequency - Inverse Document Frequency)__\n\nWe will use extracted features from both one by one to perform sentiment analysis and will compare the result at last.\n\nCheck out my below kernel to properly understand these techniques:<br/>\n__https://www.kaggle.com/amar09/text-pre-processing-and-feature-extraction__", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "ac943fa645c528c6cee36de60e9fbdd8be384cdd"
            }
        }, 
        {
            "source": "### <a id='5A'>A. Feature Extraction for 'Key Words'</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "622ce8acd9e96e232d3a9b6d526ee7ca675f5a55"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "a92ca10bbce43b9a01faef3dff8b6bc5da5f8c6b"
            }, 
            "outputs": [], 
            "source": "# BOW features\nbow_word_vectorizer = CountVectorizer(max_df=0.90, min_df=2, stop_words='english')\n# bag-of-words feature matrix\nbow_word_feature = bow_word_vectorizer.fit_transform(tweets_df['absolute_tidy_tweets'])\n\n# TF-IDF features\ntfidf_word_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, stop_words='english')\n# TF-IDF feature matrix\ntfidf_word_feature = tfidf_word_vectorizer.fit_transform(tweets_df['absolute_tidy_tweets'])"
        }, 
        {
            "source": "### <a id='5B'>B. Feature Extraction for 'Key Phrases'</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "d260fecbc21fc8d125dd2c2bd4399ad89b24c943"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "76e9299e909ac957ea0d99a570c670a1c3e580e2"
            }, 
            "outputs": [], 
            "source": "phrase_sents = tweets_df['key_phrases'].apply(lambda x: ' '.join(x))\n\n# BOW phrase features\nbow_phrase_vectorizer = CountVectorizer(max_df=0.90, min_df=2, stop_words='english')\nbow_phrase_feature = bow_phrase_vectorizer.fit_transform(phrase_sents)\n\n# TF-IDF phrase feature\ntfidf_phrase_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, stop_words='english')\ntfidf_phrase_feature = tfidf_phrase_vectorizer.fit_transform(phrase_sents)"
        }, 
        {
            "source": "## <a id='6'>6. Model Building: Sentiment Analysis</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "8c374d7527da7c24177cc42e3c2b3b1a87c9e38a"
            }
        }, 
        {
            "source": "#### Map target variables to  {0, 1}", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "ad9171efab73b4305acb6f29de3519285fbf9448"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "f4ec7bc53e1233f856e5d5fe60d2681d88f881b1"
            }, 
            "outputs": [], 
            "source": "target_variable = tweets_df['sentiment'].apply(lambda x: 0 if x=='negative' else 1 )"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "fa80e43c66c8ece8e5c112cca162d3b0821e9eb0"
            }, 
            "outputs": [], 
            "source": "def plot_confusion_matrix(matrix):\n    plt.clf()\n    plt.imshow(matrix, interpolation='nearest', cmap=plt.cm.Set2_r)\n    classNames = ['Positive', 'Negative']\n    plt.title('Confusion Matrix')\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    tick_marks = np.arange(len(classNames))\n    plt.xticks(tick_marks, classNames)\n    plt.yticks(tick_marks, classNames)\n    s = [['TP','FP'], ['FN', 'TN']]\n\n    for i in range(2):\n        for j in range(2):\n            plt.text(j,i, str(s[i][j])+\" = \"+str(matrix[i][j]))\n    plt.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "db52e07c48dea6ac9b2b7dde0efc6bdfca3d9dbd"
            }, 
            "outputs": [], 
            "source": "def naive_model(X_train, X_test, y_train, y_test):\n    naive_classifier = GaussianNB()\n    naive_classifier.fit(X_train.toarray(), y_train)\n\n    # predictions over test set\n    predictions = naive_classifier.predict(X_test.toarray())\n    \n    # calculating f1 score\n    print(f'F1 Score - {f1_score(y_test, predictions)}')\n    conf_matrix = confusion_matrix(y_test, predictions, labels=[True, False])\n    plot_confusion_matrix(conf_matrix)"
        }, 
        {
            "source": "### <a id='6A'>A. Predictions on 'key words' based features</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "626ab55ed5aa251f2b4c7ac731fbf6dcf81eef8d"
            }
        }, 
        {
            "source": "#### <a id='6Aa'>a. BOW word features</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "e9f8b75872033b9a57d5eb6a1a4bfe79d21fbfe8"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "dcf0014d2d0bec1762c4e9a087b9487e1b34684c"
            }, 
            "outputs": [], 
            "source": "X_train, X_test, y_train, y_test = train_test_split(bow_word_feature, target_variable, test_size=0.3, random_state=870)\nnaive_model(X_train, X_test, y_train, y_test)"
        }, 
        {
            "source": "#### <a id='6Ab'>b. TF-IDF word features</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "69780243fdc12097c09272d42cf96046fda22a25"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "bae6f1c0864306af8264bcd4e74ab2d8ab9a8c17"
            }, 
            "outputs": [], 
            "source": "X_train, X_test, y_train, y_test = train_test_split(tfidf_word_feature, target_variable, test_size=0.3, random_state=870)\nnaive_model(X_train, X_test, y_train, y_test)"
        }, 
        {
            "source": "### <a id='6B'>B. Predictions on 'key phrases' based features</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "e577822dd20c868489092c7681d9223b510092da"
            }
        }, 
        {
            "source": "#### <a id='6Ba'>a. BOW Phrase features</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "978df7935872c2db97b13afe4cca823e824cc27e"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "025a289b94c9a7a94169c3dd87b54bcd36b0d50e"
            }, 
            "outputs": [], 
            "source": "X_train, X_test, y_train, y_test = train_test_split(bow_phrase_feature, target_variable, test_size=0.3, random_state=870)\nnaive_model(X_train, X_test, y_train, y_test)"
        }, 
        {
            "source": "#### <a id='6Bb'>b. TF-IDF Phrase features</a>", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "b4a95ca3ecbe3b8da77eaa9d9834878be0bde874"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "8b97b458d199b94559d1b1b3c663d60cb683df07"
            }, 
            "outputs": [], 
            "source": "X_train, X_test, y_train, y_test = train_test_split(tfidf_phrase_feature, target_variable, test_size=0.3, random_state=870)\nnaive_model(X_train, X_test, y_train, y_test)"
        }, 
        {
            "source": "Features extracted from 'key words' helps model in performing better. They have better positive predictions than the later one. However, using 'key-phrase' based features improves the negative predictions.<br/><br/>\n__This is it from my side, suggestions are always welcome. Thanks :)__", 
            "cell_type": "markdown", 
            "metadata": {
                "_uuid": "e9eefd1af1e0ff79d438273fc29ba06dcce08c08"
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_uuid": "685edd427c106e2ea7f31710cdc6b91ca91eb576"
            }, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}